{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6IoX6Z5-GRq"
      },
      "source": [
        "# Manual Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tntDy8uk8JTT",
        "outputId": "051a30a3-2841-4ef0-96b9-d5840687e037"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Loss: 2.4504393709811434\n",
            "Epoch 8/100, Loss: 1.7719853101746554\n",
            "Epoch 15/100, Loss: 1.1776729908183021\n",
            "Epoch 22/100, Loss: 0.925153318650677\n",
            "Epoch 29/100, Loss: 0.7375225789630429\n",
            "Epoch 36/100, Loss: 0.6481489155949324\n",
            "Epoch 43/100, Loss: 0.5371172326222754\n",
            "Epoch 50/100, Loss: 0.5206745466747266\n",
            "Epoch 57/100, Loss: 0.4560617990472848\n",
            "Epoch 64/100, Loss: 0.41128787881966994\n",
            "Epoch 71/100, Loss: 0.3863139514802078\n",
            "Epoch 78/100, Loss: 0.3698284780939989\n",
            "Epoch 85/100, Loss: 0.357207156428861\n",
            "Epoch 92/100, Loss: 0.34702322451289874\n",
            "Epoch 99/100, Loss: 0.33847161524275926\n",
            "Manual Implementation Accuracy: 90.22%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def initialize_weights(input_size, hidden_size, output_size):\n",
        "    W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1. / input_size)\n",
        "    b1 = np.zeros((1, hidden_size))\n",
        "    W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1. / hidden_size)\n",
        "    b2 = np.zeros((1, output_size))\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z))\n",
        "    return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
        "\n",
        "def feed_forward(X, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return A1, A2\n",
        "\n",
        "# Cross-entropy loss\n",
        "def compute_loss(Y, A2):\n",
        "    m = Y.shape[0]\n",
        "    log_likelihood = -np.log(A2[range(m), np.argmax(Y, axis=1)])\n",
        "    loss = np.sum(log_likelihood) / m\n",
        "    return loss\n",
        "\n",
        "def back_propagation(X, Y, W1, b1, W2, b2, A1, A2):\n",
        "    m = X.shape[0]\n",
        "    dZ2 = A2 - Y\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * A1 * (1 - A1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist['data'], mnist['target'].astype(int)\n",
        "\n",
        "X = X / 255.0\n",
        "\n",
        "y = np.array(y)\n",
        "encoder = OneHotEncoder(categories='auto')\n",
        "y_one_hot = encoder.fit_transform(y.reshape(-1, 1)).toarray()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=10000, train_size=60000, random_state=42)\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 784\n",
        "hidden_size = 100\n",
        "output_size = 10\n",
        "learning_rate = 1.6\n",
        "epochs = 100\n",
        "\n",
        "W1, b1, W2, b2 = initialize_weights(input_size, hidden_size, output_size)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    A1, A2 = feed_forward(X_train, W1, b1, W2, b2)\n",
        "    loss = compute_loss(y_train, A2)\n",
        "    dW1, db1, dW2, db2 = back_propagation(X_train, y_train, W1, b1, W2, b2, A1, A2)\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    if epoch % 7 == 0:  # Print loss every 5 epochs\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n",
        "\n",
        "# Evaluate the model\n",
        "_, A2_test = feed_forward(X_test, W1, b1, W2, b2)\n",
        "predictions = np.argmax(A2_test, axis=1)\n",
        "labels = np.argmax(y_test, axis=1)\n",
        "manual_accuracy = np.mean(predictions == labels)\n",
        "print(f'Manual Implementation Accuracy: {manual_accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InDYMJpR90Ip"
      },
      "source": [
        "## Implementation with TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFoD6Hoz9uLC",
        "outputId": "68a462d1-4c39-4ef5-ba37-b9e0edd387b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4159 - accuracy: 0.8939\n",
            "Epoch 2/15\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2009 - accuracy: 0.9424\n",
            "Epoch 3/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1487 - accuracy: 0.9578\n",
            "Epoch 4/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1165 - accuracy: 0.9666\n",
            "Epoch 5/15\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0941 - accuracy: 0.9733\n",
            "Epoch 6/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0782 - accuracy: 0.9781\n",
            "Epoch 7/15\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0661 - accuracy: 0.9818\n",
            "Epoch 8/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0564 - accuracy: 0.9844\n",
            "Epoch 9/15\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0478 - accuracy: 0.9871\n",
            "Epoch 10/15\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0412 - accuracy: 0.9896\n",
            "Epoch 11/15\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0352 - accuracy: 0.9912\n",
            "Epoch 12/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0301 - accuracy: 0.9930\n",
            "Epoch 13/15\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0258 - accuracy: 0.9938\n",
            "Epoch 14/15\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0222 - accuracy: 0.9954\n",
            "Epoch 15/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0187 - accuracy: 0.9960\n",
            "313/313 - 1s - loss: 0.0840 - accuracy: 0.9728 - 883ms/epoch - 3ms/step\n",
            "TensorFlow Implementation Accuracy: 97.28%\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist['data'], mnist['target'].astype(int)\n",
        "X = X / 255.0\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, train_size=60000, random_state=42)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=15, batch_size=32)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(f'TensorFlow Implementation Accuracy: {test_acc * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b12MjdJ0HLfK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA65say1-Z_D"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
